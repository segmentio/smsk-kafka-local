
#
# Broker configuration.
#

delete.topic.enable=true

#
# Replication.
#

replica.fetch.wait.max.ms=500
replica.high.watermark.checkpoint.interval.ms=5000
replica.socket.timeout.ms=30000
replica.lag.time.max.ms=10000
controller.socket.timeout.ms=30000
controller.message.queue.size=10

#
# When a producer sets acks to "all" (or "-1"), min.insync.replicas specifies the minimum number of replicas
# that must acknowledge a write for the write to be considered successful.
#
# If this minimum cannot be met, then the producer will raise an exception (either NotEnoughReplicas or NotEnoughReplicasAfterAppend).
#
# When used together, min.insync.replicas and acks allow you to enforce greater durability guarantees.
# A typical scenario would be to create a topic with a replication factor of 3,
# set min.insync.replicas to 2, and produce with acks of "all".
#
# This will ensure that the producer raises an exception if a majority of replicas do not receive a write.
#

min.insync.replicas=2

#
# Number of recovery threads to use
# when loading log segments/partitions from disk.
#

num.recovery.threads.per.data.dir=5

#
# Default topic configuration.
#
# The default topic replication is 3, because
# we can easily shutdown a single broker for maintainence
# while keeping two replicas up, we should make sure
# that the replicas are spread across azs.
#
# We also create 9 partitions for a topic by default
# To ensure partitions are spread equally among availability zones.
#

offsets.topic.replication.factor=3
default.replication.factor=3
num.partitions=9
message.max.bytes=1000000
auto.create.topics.enable=false

#
# Log configuration.
#

log.dir=/kafka/logs
log.index.interval.bytes=4096
log.index.size.max.bytes=10485760
log.retention.hours=6
log.roll.hours=168
log.retention.check.interval.ms=300000
log.segment.bytes=1073741824

#
# Zookeeper.
#

zookeeper.connection.timeout.ms=6000
zookeeper.sync.time.ms=2000

#
# Socket server configuration
#

socket.request.max.bytes=104857600
socket.receive.buffer.bytes=1048576
socket.send.buffer.bytes=1048576
fetch.purgatory.purge.interval.requests=100
producer.purgatory.purge.interval.requests=100

#
# start.sh generated config.
#
broker.id=0
zookeeper.connect=zookeeper:2181/
broker.rack=RACK-0
advertised.host.name=kafka-0
advertised.port=9092
auto.leader.rebalance.enable=true
auto.create.topics.enable=true
metric.reporters=com.linkedin.kafka.cruisecontrol.metricsreporter.CruiseControlMetricsReporter

listeners=PLAINTEXT://:9092
advertised.listeners=PLAINTEXT://kafka-0:9092
#listener.security.protocol.map=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT
inter.broker.listener.name=PLAINTEXT

sasl.enabled.mechanisms=PLAIN,SCRAM-SHA-256,SCRAM-SHA-512
sasl.mechanism.inter.broker.protocol=
